{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = th.randn(2)\n",
    "cs = c.shape[0]\n",
    "n = th.randn(4)\n",
    "ns = n.shape[0]\n",
    "l = th.randn(3)\n",
    "r = th.randn(3)\n",
    "ss = l.shape[0]\n",
    "assert l.shape[0] == r.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4235,  0.2918])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(nn.Module):\n",
    "    __constants__ = ['sbias', 'cbias']\n",
    "    __weights__ = ['c2s', 'c2c', 's2c', 's1s', 's2s']\n",
    "\n",
    "    def __init__(self, in_const, in_side, out_const, out_side):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_const = in_const\n",
    "        self.in_side = in_side\n",
    "        self.out_const = out_const\n",
    "        self.out_side = out_side\n",
    "        \n",
    "        self.c2s = Parameter(th.Tensor(out_side, in_const))\n",
    "        self.s1s = Parameter(th.Tensor(out_side, in_side))\n",
    "        self.s2s = Parameter(th.Tensor(out_side, in_side))\n",
    "        self.sbias = Parameter(th.Tensor(out_side))\n",
    "        \n",
    "        self.s2c = Parameter(th.Tensor(out_const, in_side))\n",
    "        self.c2c = Parameter(th.Tensor(out_const, in_const))\n",
    "        self.cbias = Parameter(th.Tensor(out_const))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    \n",
    "    def forward(self, c, l, r):\n",
    "        c2s = F.linear(c, self.c2s, self.sbias)\n",
    "        s2c = F.linear((l+r)/2, self.s2c, self.cbias)\n",
    "        return (\n",
    "            s2c + F.linear(c, self.c2c),                         # constant part\n",
    "            c2s + F.linear(l, self.s1s) + F.linear(r, self.s2s), # left\n",
    "            c2s + F.linear(r, self.s1s) + F.linear(l, self.s2s), # right\n",
    "        )\n",
    "        \n",
    "#         self.in_features = in_features\n",
    "#         self.out_features = out_features\n",
    "#         self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        \n",
    "#         if bias:\n",
    "#             self.bias = Parameter(torch.Tensor(out_features))\n",
    "#         else:\n",
    "#             self.register_parameter('bias', None)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for wname in self.__weights__:\n",
    "            init.kaiming_uniform_(getattr(self, wname), a=math.sqrt(5))\n",
    "            \n",
    "        bound = 1 / math.sqrt(self.in_const + 2 * self.in_side)\n",
    "        init.uniform_(self.sbias, -bound, bound)\n",
    "        init.uniform_(self.cbias, -bound, bound)\n",
    "#         if self.bias is not None:\n",
    "#             fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "#             bound = 1 / math.sqrt(fan_in)\n",
    "#             init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "#     @weak_script_method\n",
    "#     def forward(self, input):\n",
    "#         return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "#     def extra_repr(self):\n",
    "#         return 'in_features={}, out_features={}, bias={}'.format(\n",
    "#             self.in_features, self.out_features, self.bias is not None\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, c_in, n_in, s_in, c_out, s_out, num_layers=3, num_hidden=16):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.s_in = s_in\n",
    "        self.n_in = n_in\n",
    "        self.layers = []\n",
    "        last_cin = c_in + n_in\n",
    "        last_sin = s_in\n",
    "        for i in range(num_layers-1):\n",
    "            self.layers.append(MyLayer(last_cin, last_sin, num_hidden, num_hidden))\n",
    "            self.add_module(\"layer%d\" % i, self.layers[i])\n",
    "            last_cin, last_sin = num_hidden, num_hidden\n",
    "        self.layers.append(MyLayer(last_cin, last_sin, c_out, s_out))\n",
    "        self.add_module(\"final\", self.layers[-1])\n",
    "    \n",
    "    def _forward_one_side(self, c, l, r):\n",
    "        for layer in self.layers:\n",
    "            c, l, r = layer(c, l, r)\n",
    "        return th.cat([c, l, r], -1)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        # TODO: fix for batch or multi\n",
    "        c = obs[         :self.c_in]\n",
    "        n = obs[self.c_in:self.c_in+self.n_in]\n",
    "        l = obs[self.c_in+self.n_in:self.c_in+self.n_in+self.s_in]\n",
    "        r = obs[-self.s_in:]\n",
    "        return (\n",
    "              self._forward_one_side(th.cat([c, n], -1), l, r)\n",
    "            + self._forward_one_side(th.cat([c, -n], -1), l, r)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNet(cs, ns, ss, 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3242,  0.0867,  0.1860,  2.3088,  0.9979,  0.3886, -0.3321, -0.7505],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(th.cat([c, n, l, r], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9979,  0.3886, -0.3321, -0.7505, -2.3242,  0.0867,  0.1860,  2.3088],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(th.cat([c, -n, r, l], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negatives\n",
    "This doesn't handle them correctly since we have $f(c, n, l, r) = f(c, -n, l, r)$ as well\n",
    "\n",
    "Correct way:\n",
    " - negatives on sides: just invert them in input\n",
    " - fixed negatives: add to layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SymmetricLayer(nn.Module):\n",
    "    __constants__ = [\"sbias\", \"cbias\"]\n",
    "    __weights__ = [\"c2s\", \"n2s\", \"c2c\", \"s2c\", \"s1s\", \"s2s\", \"n2n\", \"s2n\"]\n",
    "\n",
    "    def __init__(self, in_const, in_neg, in_side, out_const, out_neg, out_side):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_const = in_const\n",
    "        self.in_neg = in_neg\n",
    "        self.in_side = in_side\n",
    "        self.out_const = out_const\n",
    "        self.out_neg = out_neg\n",
    "        self.out_side = out_side\n",
    "\n",
    "        self.c2s = Parameter(th.Tensor(out_side, in_const))\n",
    "        self.s1s = Parameter(th.Tensor(out_side, in_side))\n",
    "        self.s2s = Parameter(th.Tensor(out_side, in_side))\n",
    "        self.n2s = Parameter(th.Tensor(out_side, in_neg))\n",
    "        self.sbias = Parameter(th.Tensor(out_side))\n",
    "\n",
    "        self.s2c = Parameter(th.Tensor(out_const, in_side))\n",
    "        self.c2c = Parameter(th.Tensor(out_const, in_const))\n",
    "        self.cbias = Parameter(th.Tensor(out_const))\n",
    "\n",
    "        self.n2n = Parameter(th.Tensor(out_neg, in_neg))\n",
    "        self.s2n = Parameter(th.Tensor(out_neg, in_side))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, c, n, l, r):\n",
    "        c2s = F.linear(c, self.c2s, self.sbias)\n",
    "        n2s = F.linear(n, self.n2s)\n",
    "        s2c = F.linear((l + r) / 2, self.s2c, self.cbias)\n",
    "        s2n = F.linear(l - r, self.s2n)\n",
    "        return (\n",
    "            s2c + F.linear(c, self.c2c),  # constant part\n",
    "            s2n + F.linear(n, self.n2n),  # negative part\n",
    "            c2s + n2s + F.linear(l, self.s1s) + F.linear(r, self.s2s),  # left\n",
    "            c2s - n2s + F.linear(r, self.s1s) + F.linear(l, self.s2s),  # right\n",
    "        )\n",
    "\n",
    "    #         self.in_features = in_features\n",
    "    #         self.out_features = out_features\n",
    "    #         self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "\n",
    "    #         if bias:\n",
    "    #             self.bias = Parameter(torch.Tensor(out_features))\n",
    "    #         else:\n",
    "    #             self.register_parameter('bias', None)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for wname in self.__weights__:\n",
    "            init.kaiming_uniform_(getattr(self, wname), a=math.sqrt(5))\n",
    "\n",
    "        bound = 1 / math.sqrt(self.in_const + self.in_neg + 2 * self.in_side)\n",
    "        init.uniform_(self.sbias, -bound, bound)\n",
    "        init.uniform_(self.cbias, -bound, bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricNet(nn.Module):\n",
    "    def __init__(self, c_in, n_in, s_in, c_out, n_out, s_out, num_layers=3, num_hidden=16, tanh_finish=True,\n",
    "        varying_std=False,\n",
    "        log_std=-1,\n",
    "        deterministic=False,):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.s_in = s_in\n",
    "        self.n_in = n_in\n",
    "        self.tanh_finish = tanh_finish\n",
    "        self.deterministic = deterministic\n",
    "        \n",
    "        self.layers = []\n",
    "        last_cin = c_in\n",
    "        last_nin = n_in\n",
    "        last_sin = s_in\n",
    "        for i in range(num_layers-1):\n",
    "            self.layers.append(SymmetricLayer(last_cin, last_nin, last_sin, num_hidden, num_hidden, num_hidden))\n",
    "            self.add_module(\"layer%d\" % i, self.layers[i])\n",
    "            last_cin, last_nin, last_sin = num_hidden, num_hidden, num_hidden\n",
    "        self.layers.append(SymmetricLayer(last_cin, last_nin, last_sin, c_out, n_out, s_out))\n",
    "        self.add_module(\"final\", self.layers[-1])\n",
    "        \n",
    "        if not self.deterministic:\n",
    "            action_size = c_out + n_out + 2 * s_out\n",
    "            if varying_std:\n",
    "                self.log_std_param = nn.Parameter(\n",
    "                    th.randn(action_size) * 1e-10 + log_std\n",
    "                )\n",
    "            else:\n",
    "                self.log_std_param = log_std * th.ones(action_size)\n",
    "\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        # TODO: better fix than transpose?\n",
    "        obs = obs.transpose(0, -1)\n",
    "        c = obs[         :self.c_in].transpose(0, -1)\n",
    "        n = obs[self.c_in:self.c_in+self.n_in].transpose(0, -1)\n",
    "        l = obs[self.c_in+self.n_in:self.c_in+self.n_in+self.s_in].transpose(0, -1)\n",
    "        r = obs[-self.s_in:].transpose(0, -1)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                n = th.tanh(n)   # TODO\n",
    "                c = th.relu(c)\n",
    "                r = th.relu(r)\n",
    "                l = th.relu(l)\n",
    "\n",
    "            c, n, l, r = layer(c, n, l, r)\n",
    "\n",
    "        mean = th.cat([c, n, l, r], -1)\n",
    "        if self.tanh_finish:\n",
    "            mean = th.tanh(mean)\n",
    "        \n",
    "        if not self.deterministic:\n",
    "            log_std = self.log_std_param.expand_as(mean)\n",
    "            return mean, log_std\n",
    "        else:\n",
    "            return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SymmetricNet(cs, ns, ss, 0, 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0825, -0.1993,  0.0163,  0.0147,  0.0537,  0.1788,  0.3346, -0.7536],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([-1., -1., -1., -1., -1., -1., -1., -1.]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(th.cat([c, n, l, r], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0537,  0.1788,  0.3346, -0.7536,  0.0825, -0.1993,  0.0163,  0.0147],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([-1., -1., -1., -1., -1., -1., -1., -1.]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(th.cat([c, -n, r, l], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    " - The final layer can have a bias for the negative part, but it wouldn't really be useful anyway\n",
    " - anything to replace `transpose`?\n",
    " - Investigate why I can't use `relu` for `n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
